{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sentence_transformers import util\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#json string data\n",
    "employee_string = '[{\"first_name\": \"Michael\", \"last_name\": \"Rodgers\", \"department\": \"Marketing\"}]'\n",
    "\n",
    "#check data type with type() method\n",
    "print(type(employee_string))\n",
    "\n",
    "#convert string to  object\n",
    "json_object = json.loads(employee_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"role\": \"user\", \"content\": \"valami\"}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'valami'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = []\n",
    "history.append({\"role\": \"user\", \"content\": \"valami\"})\n",
    "history = json.dumps(history)\n",
    "print(history)\n",
    "json.loads(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_checkker(question, refq, treshold=0.5, model=None):\n",
    "    if model is not None:\n",
    "        emb_refq = model.encode(refq)\n",
    "        emb_actq = model.encode([question])\n",
    "\n",
    "        cosine_similarities = util.cos_sim(emb_refq, emb_actq)\n",
    "\n",
    "        return cosine_similarities.mean() > treshold\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_stream(batch):\n",
    "    if len(batch['text_output']) == 0:\n",
    "        return False\n",
    "    elif batch['text_output'] == \"\\n\\n\":\n",
    "        return False\n",
    "    elif \"<|start_header_id|>\" in batch['text_output']:\n",
    "        return False\n",
    "    elif \"assistant\" in batch['text_output']:\n",
    "        return False\n",
    "    elif \"<|end_header_id|>\" in batch['text_output']:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{config['sources']}/total.json\", 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://172.22.214.120:28800/v2/models/llama-3.2-1b-instruct/generate_stream'"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f\"{config['triton']['host']}:{config['triton']['port']}/v2/{config['triton']['model']}/{config['triton']['generation']}\"\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f394b9e4fb416aa2996753d3f6e595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lrobert\\anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lrobert\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1b-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbbefd8ce9948a0a50fc3dd8152db4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f22645854f42e492eeacf44ac147fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86accb8933d246259aea3e7a1ea6cffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Name\t\t:  meta-llama/Llama-3.2-1b-instruct \n",
      " Max token input size:\t:  131072\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model repository\"])\n",
    "autoconfig = AutoConfig.from_pretrained(config[\"model repository\"])\n",
    "\n",
    "print(\" Model Name\\t\\t: \", config[\"model repository\"],\"\\n\", \"Max token input size:\\t: \", autoconfig.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_st = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mennyi volt az átlagos páratartalom 2024.09.16-án 10 órakkor.'"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question = \"Mennyi volt az átlagos páratartalom az utolsó 24 órában?\"\n",
    "question = \"Mennyi volt az átlagos páratartalom 2024.09.16-án 10 órakkor.\"\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sajnálom erre a kérdésre nem áll rendelkezésre információ.\n"
     ]
    }
   ],
   "source": [
    "if question_checkker(question=question, \n",
    "                    refq=config[\"filters\"][\"reference questions\"], \n",
    "                    treshold=config[\"filters\"][\"treshold\"], \n",
    "                    model=model_st):\n",
    "    print(\"Ok\")\n",
    "else:\n",
    "    print(config[\"guard\"][\"no information\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11248"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce = \" Csak a kérdésre válaszolj! Ne magyarázd a válaszod és ne sorolj fel részadatokat de teljes mondatban válaszolj!\"\n",
    "chat = config[\"chat history\"].copy()\n",
    "chat[-1][\"content\"] = f\"Páratartalom adatai (JSON):{str(data)}\\n\\n\"\n",
    "chat.append({\"role\": \"user\", \"content\": question + reinforce})\n",
    "\n",
    "message = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "len(tokenizer.encode(message))                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024.09.12 1 óra/h: '48.33%', 2024.09.13 2 óra/h: '49.22%', 2024.09.14 3 óra/h: '47.914%', 2024.09.15 4 óra/h: '48.215%', 2024.09.16 5 óra/h: '44.258%'"
     ]
    }
   ],
   "source": [
    "if autoconfig.max_position_embeddings > len(tokenizer.encode(message)):\n",
    "    payload = {\n",
    "        \"text_input\": message,\n",
    "        \"max_tokens\": config['triton']['max_tokens'],\n",
    "        \"temperature\": config['triton']['temperature'],\n",
    "        \"stream\": (\"stream\" in config['triton']['generation'])\n",
    "    }\n",
    "\n",
    "    # Kérelem küldése és stream feldolgozása\n",
    "    with requests.post(url, json=payload, stream=True) as response:\n",
    "        # Ellenőrizzük, hogy sikeres volt-e a válasz\n",
    "        if response.status_code == 200:\n",
    "            # Streameljük a válasz sorokat\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    # JSON-adat soronkénti feldolgozása\n",
    "                    batch = json.loads(line.decode('utf-8').replace(\"data: \", \"\"))\n",
    "                    if cleaning_stream(batch):\n",
    "                        print(batch['text_output'], end='', flush=True)\n",
    "                    # print(data['text_output'], end='\\n', flush=True)\n",
    "        else:\n",
    "            print(f\"Hiba történt: {response.status_code}\")\n",
    "else:\n",
    "    print(\"Need cutting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning is an absolute hero of modern technology. It has numerous applications and transformed the way businesses operate today. Machine learning algorithms have been used to identify defect in products, improves supply chain efficiency by minimizing waste, and even provides personalized recommendations in customer service.\n",
      "\n",
      "With the rise of artificial intelligence and deep learning, we are witness to a new era of AI automation. This technology is being applied to various industries such as finance, healthcare, and even the delivery of food. For instance, chatbots and virtual assistants are being used to handle customer inquiries, provide healthcare services, and even assist in estimating costs for legal documents.\n",
      "\n",
      "An extra added benefit is that these artificial\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# import json\n",
    "\n",
    "# # A POST URL-je a model híváshoz\n",
    "# url = \"http://172.22.214.120:28800/v2/models/llama-3.2-1b-instruct/generate\"\n",
    "# # url = \"http://172.22.214.120:28800/v2/models/llama-3.1-8b-instruct/generate\"\n",
    "\n",
    "# # A kéréshez tartozó payload\n",
    "# payload = {\n",
    "#     \"text_input\": \"machine learning is\",\n",
    "#     \"max_tokens\": 128,\n",
    "#     \"temperature\": 0.95\n",
    "# }\n",
    "\n",
    "# # Kérelem küldése és a válasz feldolgozása\n",
    "# response = requests.post(url, json=payload)\n",
    "\n",
    "# # Ellenőrizzük, hogy sikeres volt-e a válasz\n",
    "# if response.status_code == 200:\n",
    "#     # Teljes JSON válasz feldolgozása\n",
    "#     data = response.json()\n",
    "#     print(data['text_output'])\n",
    "# else:\n",
    "#     print(f\"Hiba történt: {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
